<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="description"
        content="TransFIRA introduces a transfer learning framework for face image recognizability assessment that grounds quality prediction directly in embedding geometry. It unifies encoder-specific supervision, recognizability-informed aggregation, and explainabilityâ€”achieving state-of-the-art performance on BRIAR and IJB-C while extending to body recognition.">
    <meta name="keywords"
        content="TransFIRA, Face Image Quality Assessment, FIQA, Recognizability, CCS, CCAS, Face Recognition, BRIAR, IJB-C, Template Aggregation, Explainability, 3DGS, UMD, Johns Hopkins, Tom Goldstein, Vishal Patel">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>TransFIRA: Transfer Learning for Face Image Recognizability Assessment</title>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-CF4D7TMLZE"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());

        gtag('config', 'G-CF4D7TMLZE');
    </script>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css"
        integrity="sha512-Kc323vGBEqzTmouAECnVceyQqyqdsSiqLQISBL29aUW4U/M7pSPA/gEUZQqv1cwx4OnYxTxve5UMg5GT6L4JJg=="
        crossorigin="anonymous" referrerpolicy="no-referrer" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">
    <link rel="icon" href="./static/images/favicon.ico">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/index.js"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

</head>

<body>
    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title is-1 publication-title">TransFIRA: Transfer Learning for<br>Face Image
                            Recognizability Assessment</h1>
                        <!-- <h1 style="font-size:1.5rem">Preprint<br></h1> -->
                        <div class="is-size-5 publication-authors">
                            <span class="author-block">
                                <a href="https://tuallen.github.io">Allen Tu</a><sup>1,2</sup>,</span>
                            <span class="author-block">
                                <a href="https://kartik-3004.github.io/portfolio/">Kartik
                                    Narayan</a><sup>3</sup>,</span>
                            <span class="author-block">
                                <a href="https://scholar.google.com/citations?user=FUchtr4AAAAJ">Joshua
                                    Gleason</a><sup>1</sup>,</span>
                            <span class="author-block">
                                <a href="https://scholar.google.com/citations?user=iFy2JdkAAAAJ">Jennifer
                                    Xu</a><sup>1</sup>, </span><br>
                            <span class="author-block">
                                <a href="https://www.linkedin.com/in/matthew-meyn-579784157">Matthew
                                    Meyn</a><sup>1</sup>, </span>
                            <span class="author-block">
                                <a href="https://www.cs.umd.edu/~tomg/">Tom Goldstein</a><sup>2</sup>,</span>
                            <span class="author-block">
                                <a href="https://engineering.jhu.edu/faculty/vishal-patel/">Vishal M.
                                    Patel</a><sup>3</sup></span>
                        </div>

                        <div class="is-size-5 publication-authors">
                            <span class="author-block"><sup>1</sup>Systems and Technology Research,</span>
                            <span class="author-block"><sup>2</sup>University of Maryland, College Park,</span>
                            <span class="author-block"><sup>3</sup>Johns Hopkins Univeristy</span>
                        </div>

                        <div class="column has-text-centered">
                            <div class="publication-links">
                                <span class="link-block">
                                    <a href="" class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fas fa-file-pdf"></i>
                                        </span>
                                        <span>Coming Soon!</span>
                                    </a>
                                </span>
                                <!-- <span class="link-block">
                                    <a href="https://arxiv.org/abs/2412.00578"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="ai ai-arxiv"></i>
                                        </span>
                                        <span>arXiv</span>
                                    </a>
                                </span>
                                <span class="link-block">
                                    <a href="https://github.com/j-alex-hanson/speedy-splat"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fab fa-github"></i>
                                        </span>
                                        <span>Code</span>
                                    </a>
                                </span> -->
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="hero teaser">
        <div class="container is-max-desktop">
            <div class="hero-body">
                <img src="./static/images/teaser.png" alt="Teaser Image" style="width:100%; height:auto;">
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop">
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Abstract</h2>
                    <div class="content has-text-justified">
                        <p>
                            Face recognition in unconstrained environments such as surveillance, video, and web imagery
                            must contend with extreme variation in pose, blur, illumination, and occlusion, where
                            conventional visual quality metrics fail to predict whether inputs are truly recognizable to
                            the deployed encoder. Existing FIQA methods typically rely on visual heuristics, curated
                            annotations, or computationally intensive generative pipelines, leaving their predictions
                            detached from the encoder's decision geometry. We introduce <b>TransFIRA (Transfer
                                Learning for Face Image Recognizability Assessment)</b>, a lightweight and
                            annotation-free framework that grounds recognizability directly in embedding space.
                        </p>

                        <p>TransFIRA delivers three advances:</p>
                        <ol style="text-align: justify; list-style-type: lower-roman;">
                            <li>
                                A definition of recognizability via <b>class-center similarity (CCS)</b> and
                                <b>class-center angular separation (CCAS)</b>, yielding the first natural,
                                decision-boundary-aligned criterion for filtering and weighting.
                            </li>
                            <li>
                                A <b>recognizability-informed aggregation strategy</b> that achieves state-of-the-art
                                verification
                                accuracy on BRIAR and IJB-C while nearly doubling correlation with true recognizability,
                                all
                                without external labels, heuristics, or backbone-specific training.
                            </li>
                            <li>
                                New extensions beyond faces, including <b>encoder-grounded explainability</b> that
                                reveals how
                                degradations and subject-specific factors affect recognizability, and the first
                                <b>recognizability-aware body recognition assessment</b>.
                            </li>
                        </ol>

                        <p>
                            Experiments confirm state-of-the-art results on faces, strong performance on body
                            recognition,
                            and robustness under cross-dataset shifts. Together, <b>these contributions establish
                                TransFIRA as a unified, geometry-driven framework for recognizability assessment</b> â€”
                            encoder-specific,
                            accurate, interpretable, and extensible across modalities â€” significantly advancing FIQA in
                            accuracy, explainability, and scope.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column is-four-fifths has-text-centered">
                    <h2 class="title is-3">Method</h2>

                    <div class="content has-text-justified">
                        <p>
                            <strong>TransFIRA</strong> adapts a pretrained encoder to predict
                            <em>recognizability</em>â€”the likelihood that an image will be correctly identifiedâ€”directly
                            from the encoderâ€™s embedding geometry.
                        </p>

                        <p>The framework consists of three stages:</p>
                        <ol type="i" style="margin-left:1.5em;">
                            <li>Defining recognizability using embedding-based metrics,</li>
                            <li>Learning to predict these scores from images via a lightweight regression head, and</li>
                            <li>Applying the predictions for recognizability-informed filtering and weighting during
                                template aggregation.</li>
                        </ol>

                        <!-- Image Recognizability -->
                        <h4 class="title is-4 has-text-centered">Image Recognizability</h4>
                        <p>
                            Recognizability is defined entirely within the embedding space of the chosen encoder,
                            ensuring that it reflects the modelâ€™s actual discrimination ability rather than superficial
                            factors such as blur, illumination, or occlusion.
                            For each image \(x_i\) with embedding \(z_i = \phi(x_i)\), we compute a set of class-center
                            similarities that quantify how well the embedding aligns with its identity.
                        </p>
                        <p>
                            The <b>Class Center Angular Similarity (CCS)</b> measures how closely an embedding aligns
                            with the
                            center of its own class:
                        </p>
                        <p class="has-text-centered">
                            \( CCS_{x_i} = \frac{z_i^\top \mu_{y_i}}{\|z_i\|_2 \, \|\mu_{y_i}\|_2} \)
                        </p>
                        <p>
                            The <b>Nearest Nonmatch Class Center Angular Similarity (NNCCS)</b> measures its similarity
                            to the most
                            confusable impostor class:
                        </p>
                        <p class="has-text-centered">
                            \( NNCCS_{x_i} = \max_{\,j \neq y_i} \frac{z_i^\top \mu_j}{\|z_i\|_2 \, \|\mu_j\|_2} \)
                        </p>
                        <p>
                            Their difference defines the <b>Class Center Angular Separation (CCAS)</b>:
                        </p>
                        <p class="has-text-centered">
                            \( CCAS_{x_i} = CCS_{x_i} - NNCCS_{x_i} \)
                        </p>
                        <p>
                            A natural cutoff emerges at <em>CCAS&nbsp;&gt;&nbsp;0</em>, indicating that an embedding is
                            closer to its own
                            class center than to any impostor. This provides a principled, parameter-free definition of
                            recognizability
                            grounded in the encoderâ€™s decision geometry.
                        </p>

                        <!-- Recognizability Prediction Network -->
                        <h4 class="title is-4 has-text-centered">Recognizability Prediction Network</h4>
                        <p>
                            To predict recognizability directly from images, TransFIRA extends a pretrained backbone
                            with a lightweight
                            <b>recognizability prediction head</b> implemented as a small MLP.
                            The network outputs predicted scores for both CCS and CCAS:
                        </p>
                        <p class="has-text-centered">
                            \( \hat{\mathbf{r}}_i = [\hat{CCS}_{x_i},\, \hat{CCAS}_{x_i}]^\top = h_\psi(\phi(x_i)) \)
                        </p>
                        <p>
                            Training is performed end-to-end with mean squared error against ground-truth
                            recognizability labels derived
                            from the encoder itself. Fine-tuning both the backbone and head ensures recognizability
                            remains
                            <b>encoder-specific</b>, efficient to train, and fully aligned with the modelâ€™s internal
                            representation.
                        </p>

                        <!-- Recognizability-Informed Template Aggregation -->
                        <h4 class="title is-4 has-text-centered">Recognizability-Informed Template Aggregation</h4>
                        <p>
                            In template-based recognition benchmarks such as BRIAR and IJB-C, multiple images of a
                            subject are combined
                            into a single representation. TransFIRA uses predicted recognizability scores to guide this
                            aggregation
                            through two complementary steps:
                        </p>
                        <ul>
                            <li>
                                <b>Filtering:</b> Images with predicted <em>CCAS&nbsp;&gt;&nbsp;0</em> are retained,
                                ensuring that only
                                frames confidently recognized by the encoder contribute to the template.
                            </li>
                            <li>
                                <b>Weighting:</b> Each remaining embedding is weighted by its predicted CCS before
                                averaging,
                                emphasizing compact, reliable samples most representative of the class.
                            </li>
                        </ul>
                        <p>
                            These operations form a <b>recognizability-informed aggregation strategy</b> that is both
                            <b>interpretable</b> and <b>parameter-free</b>. Filtering ensures only geometrically valid
                            samples are
                            included, while weighting strengthens alignment with the class center, jointly improving
                            accuracy and
                            explainability.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop">
            <div class="columns is-centered has-text-centered">
                <div class="column is-full-width">
                    <h2 class="title is-3">Results</h2>
                    <div class="content has-text-justified">
                        <!-- ROC Comparison Figures -->
                        <figure>
                            <img src="./static/images/roc_comparison.png"
                                alt="ROC comparison on face recognition benchmarks" style="width:120%; height:auto;">
                            <img src="./static/images/roc_wf_comparison.png" alt="ROC comparison on WebFace ablation"
                                style="width:100%; height:auto;">
                            <figcaption class="has-text-justified" style="font-style:normal; margin-top:0.75em;">
                                Fig.&nbsp;2: <b>Overall ROC analysis.</b> Top: Template-level ROC comparisons
                                across different IQA methods; for clarity, only the strongest variant of each is shown.
                                Bottom: Ablation study illustrating the individual and combined effects of CCAS-based
                                filtering and CCS-based weighting. Metrics correspond to Table&nbsp;II, and <em>Average
                                    (Baseline)</em> denotes uniform mean aggregation.
                            </figcaption>
                        </figure>

                        <!-- TAR Results Table -->
                        <figure>
                            <img src="./static/images/tar_results.png" alt="Overall ROC performance table results"
                                style="width:100%; height:auto;">
                            <figcaption class="has-text-justified" style="font-style:normal; margin-top:0.75em;">
                                TABLE&nbsp;II: <b>Overall ROC performance.</b> TAR at fixed FMRs for BRIAR
                                Protocol&nbsp;3.1 and IJB-C using the backbones in Section&nbsp;IV-B. All baseline
                                methods perform <em>weighted</em> aggregation using FIQA scores, while our <b>CCAS
                                    Filter</b> and <b>Filter&nbsp;+&nbsp;Weight</b> apply our
                                <em>CCAS&nbsp;&gt;&nbsp;0</em> cutoff. <em>Average (Baseline)</em> denotes uniform
                                aggregation without quality weighting. For each column, the best result is <b><u>bolded
                                        and underlined</u></b>, and the second-best is <b>bolded</b>.
                            </figcaption>
                        </figure>

                        <!-- ERC AUC Results Table -->
                        <!-- <figure>
                            <img src="./static/images/erc_auc_results.png" alt="Image-level recognizability evaluation"
                                style="width:100%; height:auto;">
                            <figcaption class="has-text-justified" style="font-style:normal; margin-top:0.75em;">
                                TABLE&nbsp;III: <b>Image-level recognizability evaluation.</b> Image-level Spearman
                                correlation (SC) and FNMRâ€“ERC AUC at fixed FMRs for BRIAR&nbsp;Protocol&nbsp;3.1 and
                                IJB-C, using the backbones in Section&nbsp;Â§X.Y. SC is computed against CCS for all
                                methods except for <b>Ours (CCAS)</b>, where it is computed against CCAS. For each
                                dataset and backbone, the method with the highest SC or lowest AUC is highlighted in
                                <b><u>bold and underlined</u></b>, while the second-best is shown in <b>bold</b>.
                            </figcaption>
                        </figure> -->

                        <!-- Body Results Table -->
                        <figure>
                            <img src="./static/images/body_results.png" alt="Image-level recognizability evaluation"
                                style="width:50%; height:auto;">
                            <figcaption class="has-text-justified" style="font-style:normal; margin-top:0.75em;">
                                TABLE&nbsp;IV: <b>TAR at fixed FMRs for body recognition on BRIAR Protocol 3.1 using the
                                    SemReID encoder.</b> The method with the best performance for each operating point
                                is <b>bolded</b>. Full results are reported in Appendix&nbsp;B.
                            </figcaption>
                        </figure>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section" id="BibTeX">
        <div class="container is-max-desktop content">
            <h2 class="title">BibTeX</h2>
            <pre><code>@article{Tu2025TransFIRA,
    author  = {Tu, Allen and Narayan, Kartik and Gleason, Joshua and Xu, Jennifer and Meyn, Matthew and Goldstein, Tom and Patel, Vishal M.},
    title   = {TransFIRA: Transfer Learning for Face Image Recognizability Assessment},
    journal = {Preprint},
    year    = {2025},
    url     = {https://transfira.github.io/}
}</code></pre>
        </div>
    </section>

    <footer class="footer">
        <div class="container">
            <div class="columns is-centered">
                <div class="column is-8">
                    <div class="content">
                        <p>
                            This research is based upon work supported in part by the Office of the Director of National
                            Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via
                            [2022-21102100005]. The views and conclusions contained herein are those of the authors and
                            should not be interpreted as necessarily representing the official policies, either
                            expressed or implied, of ODNI, IARPA, or the U.S. Government. The US Government is
                            authorized to reproduce and distribute reprints for governmental purposes notwithstanding
                            any copyright annotation therein.
                        </p>
                    </div>
                    <div class="content has-text-centered">
                        <p>
                            We thank the authors of <a rel="nerfies"
                                href="https://github.com/nerfies/nerfies.github.io/tree/main">Nerfies</a>
                            for generously open-sourcing the templates used in this website.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </footer>

</body>

</html>