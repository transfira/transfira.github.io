<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="description"
        content="TransFIRA is a transfer learning framework for face image recognizability assessment that aligns quality prediction with encoder geometry. It achieves state-of-the-art FIQA accuracy and explainability on BRIAR and IJB-C, and extends to body recognition.">
    <meta name="keywords"
        content="TransFIRA, Face Image Quality Assessment, FIQA, Recognizability Prediction, Class Center Similarity, CCS, CCAS, Transfer Learning, Face Recognition, Body Recognition, Template Aggregation, Explainability, Embedding Geometry, Encoder-grounded, BRIAR, IJB-C, SemReID, Computer Vision, Biometrics, Deep Learning, UMD, Johns Hopkins, Tom Goldstein, Vishal Patel">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>TransFIRA: Transfer Learning for Face Image Recognizability Assessment</title>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-CF4D7TMLZE"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());

        gtag('config', 'G-CF4D7TMLZE');
    </script>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css"
        integrity="sha512-Kc323vGBEqzTmouAECnVceyQqyqdsSiqLQISBL29aUW4U/M7pSPA/gEUZQqv1cwx4OnYxTxve5UMg5GT6L4JJg=="
        crossorigin="anonymous" referrerpolicy="no-referrer" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">
    <link rel="icon" href="./static/images/favicon.ico">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/index.js"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

</head>

<body>
    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title is-1 publication-title">TransFIRA: Transfer Learning for<br>Face Image
                            Recognizability Assessment</h1>
                        <div class="is-size-5 publication-authors">
                            <span class="author-block">
                                <a href="https://tuallen.github.io">Allen Tu</a><sup>1,2</sup>,</span>
                            <span class="author-block">
                                <a href="https://kartik-3004.github.io/portfolio/">Kartik
                                    Narayan</a><sup>3</sup>,</span>
                            <span class="author-block">
                                <a href="https://scholar.google.com/citations?user=FUchtr4AAAAJ">Joshua
                                    Gleason</a><sup>1</sup>,</span>
                            <span class="author-block">
                                <a href="https://scholar.google.com/citations?user=iFy2JdkAAAAJ">Jennifer
                                    Xu</a><sup>1</sup>, </span><br>
                            <span class="author-block">
                                <a href="https://www.linkedin.com/in/matthew-meyn-579784157">Matthew
                                    Meyn</a><sup>1</sup>, </span>
                            <span class="author-block">
                                <a href="https://www.cs.umd.edu/~tomg/">Tom Goldstein</a><sup>2</sup>,</span>
                            <span class="author-block">
                                <a href="https://engineering.jhu.edu/faculty/vishal-patel/">Vishal M.
                                    Patel</a><sup>3</sup></span>
                        </div>

                        <div class="is-size-5 publication-authors">
                            <span class="author-block"><sup>1</sup>Systems and Technology Research,</span>
                            <span class="author-block"><sup>2</sup>University of Maryland, College Park,</span>
                            <span class="author-block"><sup>3</sup>Johns Hopkins University</span>
                        </div>

                        <div class="column has-text-centered">
                            <div class="publication-links">
                                <span class="link-block">
                                    <a href="https://arxiv.org/pdf/2510.06353"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fas fa-file-pdf"></i>
                                        </span>
                                        <span>Paper</span>
                                    </a>
                                </span>
                                <span class="link-block">
                                    <a href="https://arxiv.org/abs/2510.06353"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="ai ai-arxiv"></i>
                                        </span>
                                        <span>arXiv</span>
                                    </a>
                                </span>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="hero teaser">
        <div class="container is-max-desktop">
            <div class="hero-body">
                <picture>
                    <source srcset="./static/images/teaser.webp" type="image/webp">
                    <img src="./static/images/teaser.png" alt="Teaser" loading="lazy" style="width:100%;height:auto;">
                </picture>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop">
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Abstract</h2>
                    <div class="content has-text-justified">
                        <p>
                            Face recognition in unconstrained environments such as surveillance, video, and web imagery
                            must contend with extreme variation in pose, blur, illumination, and occlusion, where
                            conventional visual quality metrics fail to predict whether inputs are truly recognizable to
                            the deployed encoder. Existing FIQA methods typically rely on visual heuristics, curated
                            annotations, or computationally intensive generative pipelines, leaving their predictions
                            detached from the encoder's decision geometry. We introduce TransFIRA (Transfer Learning for
                            Face Image Recognizability Assessment), a lightweight and annotation-free framework that
                            grounds recognizability directly in embedding space.
                        </p>

                        <p>TransFIRA delivers three advances:</p>
                        <ol style="text-align: justify; list-style-type: lower-roman;">
                            <li>
                                A definition of recognizability via class-center similarity (CCS) and class-center
                                angular separation (CCAS), yielding the first natural, decision-boundary-aligned
                                criterion for filtering and weighting.
                            </li>
                            <li>
                                A recognizability-informed aggregation strategy that achieves state-of-the-art
                                verification accuracy on BRIAR and IJB-C while nearly doubling correlation with true
                                recognizability, all without external labels, heuristics, or backbone-specific training.
                            </li>
                            <li>
                                New extensions beyond faces, including encoder-grounded explainability that reveals how
                                degradations and subject-specific factors affect recognizability, and the first
                                recognizability-aware body recognition assessment.
                            </li>
                        </ol>

                        <p>
                            Experiments confirm state-of-the-art results on faces, strong performance on body
                            recognition,
                            and robustness under cross-dataset shifts. Together, these contributions establish TransFIRA
                            as a unified, geometry-driven framework for recognizability assessment — encoder-specific,
                            accurate, interpretable, and extensible across modalities — significantly advancing FIQA in
                            accuracy, explainability, and scope.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column is-four-fifths has-text-centered">
                    <h2 class="title is-3">Method</h2>

                    <div class="content has-text-justified">
                        <p>
                            <strong>TransFIRA</strong> adapts a pretrained encoder to predict
                            <em>recognizability</em>—the likelihood that an image will be correctly identified—directly
                            from the encoder’s embedding geometry.
                        </p>

                        <p>The framework consists of three stages:</p>
                        <ol type="i" style="margin-left:1.5em;">
                            <li>Defining recognizability using embedding-based metrics,</li>
                            <li>Learning to predict these scores from images via a lightweight regression head, and</li>
                            <li>Applying the predictions for recognizability-informed filtering and weighting during
                                template aggregation.</li>
                        </ol>

                        <h4 class="title is-4 has-text-centered">Image Recognizability</h4>
                        <p>
                            Recognizability is defined entirely within the embedding space of the chosen encoder,
                            ensuring that it reflects the model’s actual discrimination ability rather than superficial
                            factors such as blur, illumination, or occlusion.
                            For each image \(x_i\) with embedding \(z_i = \phi(x_i)\), we compute a set of class-center
                            similarities that quantify how well the embedding aligns with its identity.
                        </p>
                        <p>
                            The <b>Class Center Angular Similarity (CCS)</b> measures how closely an embedding aligns
                            with the
                            center of its own class:
                        </p>
                        <p class="has-text-centered">
                            \( CCS_{x_i} = \frac{z_i^\top \mu_{y_i}}{\|z_i\|_2 \, \|\mu_{y_i}\|_2} \)
                        </p>
                        <p>
                            The <b>Nearest Nonmatch Class Center Angular Similarity (NNCCS)</b> measures its similarity
                            to the most
                            confusable impostor class:
                        </p>
                        <p class="has-text-centered">
                            \( NNCCS_{x_i} = \max_{\,j \neq y_i} \frac{z_i^\top \mu_j}{\|z_i\|_2 \, \|\mu_j\|_2} \)
                        </p>
                        <p>
                            Their difference defines the <b>Class Center Angular Separation (CCAS)</b>:
                        </p>
                        <p class="has-text-centered">
                            \( CCAS_{x_i} = CCS_{x_i} - NNCCS_{x_i} \)
                        </p>
                        <p>
                            A natural cutoff emerges at <em>CCAS&nbsp;&gt;&nbsp;0</em>, indicating that an embedding is
                            closer to its own
                            class center than to any impostor. This provides a principled, parameter-free definition of
                            recognizability
                            grounded in the encoder’s decision geometry.
                        </p>

                        <h4 class="title is-4 has-text-centered">Recognizability Prediction Network</h4>
                        <p>
                            To predict recognizability directly from images, TransFIRA extends a pretrained backbone
                            with a lightweight
                            <b>recognizability prediction head</b> implemented as a small MLP.
                            The network outputs predicted scores for both CCS and CCAS:
                        </p>
                        <p class="has-text-centered">
                            \( \hat{\mathbf{r}}_i = [\hat{CCS}_{x_i},\, \hat{CCAS}_{x_i}]^\top = h_\psi(\phi(x_i)) \)
                        </p>
                        <p>
                            Training is performed end-to-end with mean squared error against ground-truth
                            recognizability labels derived from the encoder itself. Fine-tuning both the backbone and
                            head ensures recognizability remains encoder-specific, efficient to train, and fully
                            aligned with the model’s internal representation.
                        </p>

                        <h4 class="title is-4 has-text-centered">Recognizability-Informed Template Aggregation</h4>
                        <p>
                            In template-based recognition benchmarks such as BRIAR and IJB-C, multiple images of a
                            subject are combined
                            into a single representation. TransFIRA uses predicted recognizability scores to guide this
                            aggregation
                            through two complementary steps:
                        </p>
                        <ul>
                            <li>
                                <b>Filtering:</b> Images with predicted <em>CCAS&nbsp;&gt;&nbsp;0</em> are retained,
                                ensuring that only
                                frames confidently recognized by the encoder contribute to the template.
                            </li>
                            <li>
                                <b>Weighting:</b> Each remaining embedding is weighted by its predicted CCS before
                                averaging,
                                emphasizing compact, reliable samples most representative of the class.
                            </li>
                        </ul>
                        <p>
                            These operations form a recognizability-informed aggregation strategy that is both
                            interpretable and parameter-free. Filtering ensures only geometrically valid samples are
                            included, while weighting strengthens alignment with the class center, jointly improving
                            accuracy and explainability.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop">
            <div class="columns is-centered has-text-centered">
                <div class="column is-full-width">
                    <h2 class="title is-3">Results</h2>
                    <div class="content has-text-justified">
                        <figure>
                            <img src="./static/images/roc_comparison.png"
                                alt="ROC comparison on face recognition benchmarks" style="width:120%; height:auto;">
                            <img src="./static/images/roc_wf_comparison.png" alt="ROC comparison on WebFace ablation"
                                style="width:100%; height:auto;">
                            <figcaption class="has-text-justified" style="font-style:normal; margin-top:0.75em;">
                                Fig.&nbsp;2: <b>Overall ROC analysis.</b> Top: Template-level ROC comparisons
                                across different IQA methods; for clarity, only the strongest variant of each is shown.
                                Bottom: Ablation study illustrating the individual and combined effects of CCAS-based
                                filtering and CCS-based weighting. Metrics correspond to Table&nbsp;II, and <em>Average
                                    (Baseline)</em> denotes uniform mean aggregation.
                            </figcaption>
                        </figure>

                        <figure>
                            <img src="./static/images/tar_results.png" alt="Overall ROC performance table results"
                                style="width:100%; height:auto;">
                            <figcaption class="has-text-justified" style="font-style:normal; margin-top:0.75em;">
                                TABLE&nbsp;II: <b>Overall ROC performance.</b> TAR at fixed FMRs for BRIAR
                                Protocol&nbsp;3.1 and IJB-C using the backbones in Section&nbsp;IV-B. All baseline
                                methods perform <em>weighted</em> aggregation using FIQA scores, while our <b>CCAS
                                    Filter</b> and <b>Filter&nbsp;+&nbsp;Weight</b> apply our
                                <em>CCAS&nbsp;&gt;&nbsp;0</em> cutoff. <em>Average (Baseline)</em> denotes uniform
                                aggregation without quality weighting. For each column, the best result is <b><u>bolded
                                        and underlined</u></b>, and the second-best is <b>bolded</b>.
                            </figcaption>
                        </figure>

                        <!-- <figure>
                            <img src="./static/images/erc_auc_results.png" alt="Image-level recognizability evaluation"
                                style="width:100%; height:auto;">
                            <figcaption class="has-text-justified" style="font-style:normal; margin-top:0.75em;">
                                TABLE&nbsp;III: <b>Image-level recognizability evaluation.</b> Image-level Spearman
                                correlation (SC) and FNMR–ERC AUC at fixed FMRs for BRIAR&nbsp;Protocol&nbsp;3.1 and
                                IJB-C, using the backbones in Section&nbsp;§X.Y. SC is computed against CCS for all
                                methods except for <b>Ours (CCAS)</b>, where it is computed against CCAS. For each
                                dataset and backbone, the method with the highest SC or lowest AUC is highlighted in
                                <b><u>bold and underlined</u></b>, while the second-best is shown in <b>bold</b>.
                            </figcaption>
                        </figure> -->

                        <figure>
                            <img src="./static/images/body_results.png" alt="Image-level recognizability evaluation"
                                style="width:50%; height:auto;">
                            <figcaption class="has-text-justified" style="font-style:normal; margin-top:0.75em;">
                                TABLE&nbsp;IV: <b>TAR at fixed FMRs for body recognition on BRIAR Protocol 3.1 using the
                                    SemReID encoder.</b> The method with the best performance for each operating point
                                is <b>bolded</b>. Full results are reported in Appendix&nbsp;B.
                            </figcaption>
                        </figure>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section" id="BibTeX">
        <div class="container is-max-desktop content">
            <h2 class="title">BibTeX</h2>
            <pre><code>@article{Tu2025TransFIRA,
    author  = {Tu, Allen and Narayan, Kartik and Gleason, Joshua and Xu, Jennifer and Meyn, Matthew and Goldstein, Tom and Patel, Vishal M.},
    title   = {TransFIRA: Transfer Learning for Face Image Recognizability Assessment},
    journal = {arXiv preprint arXiv:2510.06353},
    year    = {2025},
    url     = {https://transfira.github.io/}
}</code></pre>
        </div>
    </section>

    <footer class="footer">
        <div class="container">
            <div class="content has-text-centered">
                <a class="icon-link" href="https://arxiv.org/pdf/2510.06353">
                    <i class="fas fa-file-pdf"></i>
                </a>
            </div>
            <div class="columns is-centered">
                <div class="column is-8">
                    <div class="content">
                        <p>
                            This research is based upon work supported in part by the Office of the Director of National
                            Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via
                            [2022-21102100005]. The views and conclusions contained herein are those of the authors and
                            should not be interpreted as necessarily representing the official policies, either
                            expressed or implied, of ODNI, IARPA, or the U.S. Government. The US Government is
                            authorized to reproduce and distribute reprints for governmental purposes notwithstanding
                            any copyright annotation therein.
                        </p>
                    </div>
                    <div class="content has-text-centered">
                        <p>
                            We thank the authors of <a rel="nerfies"
                                href="https://github.com/nerfies/nerfies.github.io/tree/main">Nerfies</a>
                            for generously open-sourcing the templates used in this website.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </footer>
</body>

</html>